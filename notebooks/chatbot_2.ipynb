{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5112,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":3900}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Final Project\n### Fine tuning of a Mistral 7B Model\n---\nMembers:\n- Bastian Castillo (C0872284)\n- Fadernel Bedoya (C0872455)\n- Marcelo Munoz (C0873813)\n- Suyog Adhikari (C0880973)\n\n**Goal**:\n\nThe goal of this proyect is to fine tune a pretrained model be able to sove basics tasks given as instructions through a chatbot interface to interact with it. This interface was going to be built using Gradio library and it will be deploy on Hugging Face Spaces. \n\n(*) Because of the limited computed power and storage resources the dataset used to fine tune the model has only 1K instances. We tried to used some other datasets use for the same purpose, but we had resources during the training and also during deployment.\n\n\nFirst of all, the required dependencies are installed in this notebook environment:","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U datasets transformers\n%pip install -U peft\n%pip install -U accelerate\n%pip install -U trl","metadata":{"execution":{"iopub.status.busy":"2023-12-11T21:56:35.461609Z","iopub.execute_input":"2023-12-11T21:56:35.461975Z","iopub.status.idle":"2023-12-11T21:57:59.345821Z","shell.execute_reply.started":"2023-12-11T21:56:35.461948Z","shell.execute_reply":"2023-12-11T21:57:59.344608Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Once the dependencies are installed, these are imported into the notebook:","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\nimport os,torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-12T01:08:54.187781Z","iopub.execute_input":"2023-12-12T01:08:54.188156Z","iopub.status.idle":"2023-12-12T01:09:21.071581Z","shell.execute_reply.started":"2023-12-12T01:08:54.188125Z","shell.execute_reply":"2023-12-12T01:09:21.070585Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Hugging Face and Wanbd Secrets\n\nHugging Face platform allows us to store the model generated and also provides a space to be deployed. W&B is a platform that facilitates the plotting of metrics during the training in a graphical way. In the next cell, we loaded the tokens for both apps as secrets to maintain them secured.","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nsecret_wandb = user_secrets.get_secret(\"WANDB_TOKEN\")","metadata":{"execution":{"iopub.status.busy":"2023-12-12T01:09:21.073712Z","iopub.execute_input":"2023-12-12T01:09:21.074754Z","iopub.status.idle":"2023-12-12T01:09:21.393260Z","shell.execute_reply.started":"2023-12-12T01:09:21.074716Z","shell.execute_reply":"2023-12-12T01:09:21.392238Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"As can be seen, it is possible to login to Hugging Face platform to then push our model to the HG repository and Space.","metadata":{}},{"cell_type":"code","source":"!huggingface-cli login --token $secret_hf","metadata":{"execution":{"iopub.status.busy":"2023-12-12T01:09:21.394377Z","iopub.execute_input":"2023-12-12T01:09:21.394702Z","iopub.status.idle":"2023-12-12T01:09:23.085774Z","shell.execute_reply.started":"2023-12-12T01:09:21.394672Z","shell.execute_reply":"2023-12-12T01:09:23.084714Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Also we access W&B platform through this notebook and setup our project graphs into this platform.","metadata":{}},{"cell_type":"code","source":"wandb.login(key = secret_wandb)\nrun = wandb.init(\n    project='Fine tuning mistral 7B', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T21:59:47.180326Z","iopub.execute_input":"2023-12-11T21:59:47.180736Z","iopub.status.idle":"2023-12-11T22:00:20.430707Z","shell.execute_reply.started":"2023-12-11T21:59:47.180701Z","shell.execute_reply":"2023-12-11T22:00:20.429558Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbascr\u001b[0m (\u001b[33mlambton-college\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231211_215949-uua0rqlj</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lambton-college/Fine%20tuning%20mistral%207B/runs/uua0rqlj' target=\"_blank\">ethereal-plant-3</a></strong> to <a href='https://wandb.ai/lambton-college/Fine%20tuning%20mistral%207B' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lambton-college/Fine%20tuning%20mistral%207B' target=\"_blank\">https://wandb.ai/lambton-college/Fine%20tuning%20mistral%207B</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lambton-college/Fine%20tuning%20mistral%207B/runs/uua0rqlj' target=\"_blank\">https://wandb.ai/lambton-college/Fine%20tuning%20mistral%207B/runs/uua0rqlj</a>"},"metadata":{}}]},{"cell_type":"markdown","source":"### Model\n\nThe model selected for the team was Mistral 7B. This model has 7.3 Billion of parameters. Despite its number of parameters it performs better than LlaMa 2 13B and LlaMa 1 34B in many benchmarks. This model has a transformer architecture. This model has two important mechanisms:\n\n- Group-query Attention: it allows a faster inference time if we compared to standard full attention.\n- Sliding Windows Attention: this means it has the ability to handle longer text sequences using less resources.\n\nWe can use this model without restriction because it was released under Apache 2.0 license.\n\nIn the following cell, the model string, the name of our fine-tuning model and the name of the dataset we will use are declared:","metadata":{}},{"cell_type":"code","source":"base_model = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\ndataset_name = \"mlabonne/guanaco-llama2-1k\"\nnew_model = \"chatbot_2\"","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:43:57.871021Z","iopub.execute_input":"2023-12-12T00:43:57.871463Z","iopub.status.idle":"2023-12-12T00:43:57.876618Z","shell.execute_reply.started":"2023-12-12T00:43:57.871426Z","shell.execute_reply":"2023-12-12T00:43:57.875550Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### Dataset\n\nThe dataset selected contains 1K of intances as instructions and the response already formatted to be used as train input into the model in multiple languages.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the dataset\ndataset = load_dataset(dataset_name,split=\"train\")\ndataset_df = pd.DataFrame(dataset)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:00:26.243557Z","iopub.execute_input":"2023-12-11T22:00:26.244463Z","iopub.status.idle":"2023-12-11T22:00:28.080610Z","shell.execute_reply.started":"2023-12-11T22:00:26.244430Z","shell.execute_reply":"2023-12-11T22:00:28.079764Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/1.02k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59eea08ac9ed4afe9d27bddbb3638885"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b12214a3afa14baabbb67e2c8071cc13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/967k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35fc173f76b842908efa98b52161022d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61b737d9a23f4c8b8098d70cce0c1af4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4605729283a4214ab501551cb011068"}},"metadata":{}}]},{"cell_type":"markdown","source":"Here can be seen some examples of the dataset. The character **\"\\<s>\"** indicates the beginning of a sentence and **\"\\</s>\"** the end of it. The text between **\"\\[INST\\]\"** and **\"\\[/INST\\]\"** correspond to an instruction, and what is out of it corresponds to the answer. Basically this data is used for Question & Answer models, like in our case.","metadata":{}},{"cell_type":"code","source":"dataset_df","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:00:31.002180Z","iopub.execute_input":"2023-12-11T22:00:31.003004Z","iopub.status.idle":"2023-12-11T22:00:31.033543Z","shell.execute_reply.started":"2023-12-11T22:00:31.002960Z","shell.execute_reply":"2023-12-11T22:00:31.032528Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                  text\n0    <s>[INST] Me gradué hace poco de la carrera de...\n1    <s>[INST] Самый великий человек из всех живших...\n2    <s>[INST] Compose a professional email with th...\n3    <s>[INST] ¿Qué juegos me recomendarías si me h...\n4    <s>[INST] Cual es el desarrollo motor de niño/...\n..                                                 ...\n995  <s>[INST] I want you to act as a Linux termina...\n996  <s>[INST] quiero un tutorial de como acceder a...\n997  <s>[INST] Auf Mani reimt sich Banani, daraus w...\n998  <s>[INST] Buenos días! [/INST] ¡Hola! ¿Cómo es...\n999  <s>[INST] Est-tu mieux que Chat-GPT? [/INST] P...\n\n[1000 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;s&gt;[INST] Me gradué hace poco de la carrera de...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;s&gt;[INST] Самый великий человек из всех живших...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;s&gt;[INST] Compose a professional email with th...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;s&gt;[INST] ¿Qué juegos me recomendarías si me h...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;s&gt;[INST] Cual es el desarrollo motor de niño/...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>&lt;s&gt;[INST] I want you to act as a Linux termina...</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>&lt;s&gt;[INST] quiero un tutorial de como acceder a...</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>&lt;s&gt;[INST] Auf Mani reimt sich Banani, daraus w...</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>&lt;s&gt;[INST] Buenos días! [/INST] ¡Hola! ¿Cómo es...</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>&lt;s&gt;[INST] Est-tu mieux que Chat-GPT? [/INST] P...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"For a faster training it is used a 4-bit precision and the model is loaded (The base_model variable contains the name of the Mistral model):","metadata":{}},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        base_model,\n        load_in_4bit=True,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n\nmodel.config.use_cache = False # silence the warnings\nmodel.config.pretraining_tp = 1\nmodel.gradient_checkpointing_enable()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:00:43.250123Z","iopub.execute_input":"2023-12-11T22:00:43.250474Z","iopub.status.idle":"2023-12-11T22:03:39.857298Z","shell.execute_reply.started":"2023-12-11T22:00:43.250444Z","shell.execute_reply":"2023-12-11T22:03:39.856028Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e715a84e704493292c72dfb0bbd9ba8"}},"metadata":{}}]},{"cell_type":"markdown","source":"Also the tokenizer is loaded and configured: EOS (end of a sentence) and padding properties.","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\ntokenizer.padding_side = 'right'\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.add_eos_token = True\ntokenizer.add_bos_token, tokenizer.add_eos_token","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:03:39.859473Z","iopub.execute_input":"2023-12-11T22:03:39.859863Z","iopub.status.idle":"2023-12-11T22:03:40.004973Z","shell.execute_reply.started":"2023-12-11T22:03:39.859821Z","shell.execute_reply":"2023-12-11T22:03:40.003847Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(True, True)"},"metadata":{}}]},{"cell_type":"markdown","source":"Due to the amount of compute power resources required for this type of text-generation models, it is necessary to reduce the number of parameters adding an adopter layer to use the memory in a more efficient way. This technique is named LoRA - Low-Rank Adaptation of Large Language Models. The approach of this technique is to represent the weights updates with two smaller matrices through low-rank decomposition. As Huggin Face documentation says: \n\n_These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined._\n\n_This approach has a number of advantages:_\n\n- _LoRA makes fine-tuning more efficient by drastically reducing the number of trainable parameters._\n- _The original pre-trained weights are kept frozen, which means you can have multiple lightweight and portable LoRA models for various downstream tasks built on top of them._\n- _LoRA is orthogonal to many other parameter-efficient methods and can be combined with many of them._\n- _Performance of models fine-tuned using LoRA is comparable to the performance of fully fine-tuned models._\n- _LoRA does not add any inference latency because adapter weights can be merged with the base model.\"_\n","metadata":{}},{"cell_type":"code","source":"model = prepare_model_for_kbit_training(model)\npeft_config = LoraConfig(\n    lora_alpha=16,\n    lora_dropout=0.1,\n    r=64,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\"]\n)\nmodel = get_peft_model(model, peft_config)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:03:40.006271Z","iopub.execute_input":"2023-12-11T22:03:40.006662Z","iopub.status.idle":"2023-12-11T22:03:41.195080Z","shell.execute_reply.started":"2023-12-11T22:03:40.006605Z","shell.execute_reply":"2023-12-11T22:03:41.193696Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Hyperparameters\n\nThe arguments for the trainer class are declared bellow, like the output local folder for the model file generated, the number of epochs (in this case, because of the resource limits we trained the model through 1 epoch), the optimizer, the number of the barhc among other hyperparameters:","metadata":{}},{"cell_type":"code","source":"training_arguments = TrainingArguments(\n    output_dir=\"./results\",\n    num_train_epochs=1,\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=1,\n    optim=\"paged_adamw_32bit\",\n    save_steps=25,\n    logging_steps=25,\n    learning_rate=2e-4,\n    weight_decay=0.001,\n    fp16=False,\n    bf16=False,\n    max_grad_norm=0.3,\n    max_steps=-1,\n    warmup_ratio=0.03,\n    group_by_length=True,\n    lr_scheduler_type=\"constant\",\n    report_to=\"wandb\"\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:03:41.197448Z","iopub.execute_input":"2023-12-11T22:03:41.198086Z","iopub.status.idle":"2023-12-11T22:03:41.208870Z","shell.execute_reply.started":"2023-12-11T22:03:41.198044Z","shell.execute_reply":"2023-12-11T22:03:41.207816Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"The trainer is instantiated and the training begging with the trainer.train() method. The model was trained around 1 hour and 30 minutes using 2 GPUS of 15GB and 16GB of RAM instance:","metadata":{}},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    train_dataset=dataset,\n    peft_config=peft_config,\n    max_seq_length= None,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing= False,\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:03:59.618169Z","iopub.execute_input":"2023-12-11T22:03:59.618888Z","iopub.status.idle":"2023-12-11T22:04:00.375657Z","shell.execute_reply.started":"2023-12-11T22:03:59.618834Z","shell.execute_reply":"2023-12-11T22:04:00.374059Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:194: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59319f623f9e466c825a442e0466b2b8"}},"metadata":{}}]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:04:08.284001Z","iopub.execute_input":"2023-12-11T22:04:08.284351Z","iopub.status.idle":"2023-12-11T23:36:13.919509Z","shell.execute_reply.started":"2023-12-11T22:04:08.284326Z","shell.execute_reply":"2023-12-11T23:36:13.918524Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 1:31:11, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>25</td>\n      <td>1.247600</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.622900</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>1.202600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.397100</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.157200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.326200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.171400</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.429200</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>1.135900</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.481000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=250, training_loss=1.317102081298828, metrics={'train_runtime': 5524.6504, 'train_samples_per_second': 0.181, 'train_steps_per_second': 0.045, 'total_flos': 1.874641569231667e+16, 'train_loss': 1.317102081298828, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"### Save model and publish to Hugging Face Hub repository\n\nThe model is locally saved and push into Hugging Face Hub to be used during the deployment into the space.","metadata":{}},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\nwandb.finish()\nmodel.config.use_cache = True","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:36:56.011498Z","iopub.execute_input":"2023-12-11T23:36:56.012432Z","iopub.status.idle":"2023-12-11T23:36:59.166777Z","shell.execute_reply.started":"2023-12-11T23:36:56.012383Z","shell.execute_reply":"2023-12-11T23:36:59.166076Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.057 MB of 0.057 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/global_step</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▃█▂▅▁▄▂▅▁▆</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>250</td></tr><tr><td>train/learning_rate</td><td>0.0002</td></tr><tr><td>train/loss</td><td>1.481</td></tr><tr><td>train/total_flos</td><td>1.874641569231667e+16</td></tr><tr><td>train/train_loss</td><td>1.3171</td></tr><tr><td>train/train_runtime</td><td>5524.6504</td></tr><tr><td>train/train_samples_per_second</td><td>0.181</td></tr><tr><td>train/train_steps_per_second</td><td>0.045</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">ethereal-plant-3</strong> at: <a href='https://wandb.ai/lambton-college/Fine%20tuning%20mistral%207B/runs/uua0rqlj' target=\"_blank\">https://wandb.ai/lambton-college/Fine%20tuning%20mistral%207B/runs/uua0rqlj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20231211_215949-uua0rqlj/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"trainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving model files","metadata":{}},{"cell_type":"code","source":"!zip -r model.zip /kaggle/working/results/adapter_config.json /kaggle/working/results/adapter_model.safetensors /kaggle/working/results/tokenizer.json /kaggle/working/results/tokenizer.model /kaggle/working/results/tokenizer_config.json /kaggle/working/results/training_args.bin /kaggle/working/results/special_tokens_map.json /kaggle/working/results/checkpoint-250","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:24:06.761704Z","iopub.execute_input":"2023-12-12T00:24:06.762173Z","iopub.status.idle":"2023-12-12T00:25:27.291634Z","shell.execute_reply.started":"2023-12-12T00:24:06.762133Z","shell.execute_reply":"2023-12-12T00:25:27.290592Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/results/adapter_config.json (deflated 50%)\n  adding: kaggle/working/results/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/results/tokenizer.json (deflated 74%)\n  adding: kaggle/working/results/tokenizer.model (deflated 55%)\n  adding: kaggle/working/results/tokenizer_config.json (deflated 64%)\n  adding: kaggle/working/results/training_args.bin (deflated 48%)\n  adding: kaggle/working/results/special_tokens_map.json (deflated 73%)\n  adding: kaggle/working/results/checkpoint-250/ (stored 0%)\n  adding: kaggle/working/results/checkpoint-250/adapter_config.json (deflated 50%)\n  adding: kaggle/working/results/checkpoint-250/tokenizer.model (deflated 55%)\n  adding: kaggle/working/results/checkpoint-250/rng_state.pth (deflated 28%)\n  adding: kaggle/working/results/checkpoint-250/tokenizer.json (deflated 74%)\n  adding: kaggle/working/results/checkpoint-250/README.md (deflated 65%)\n  adding: kaggle/working/results/checkpoint-250/scheduler.pt (deflated 51%)\n  adding: kaggle/working/results/checkpoint-250/training_args.bin (deflated 48%)\n  adding: kaggle/working/results/checkpoint-250/tokenizer_config.json (deflated 64%)\n  adding: kaggle/working/results/checkpoint-250/special_tokens_map.json (deflated 73%)\n  adding: kaggle/working/results/checkpoint-250/trainer_state.json (deflated 74%)\n  adding: kaggle/working/results/checkpoint-250/adapter_model.safetensors (deflated 8%)\n  adding: kaggle/working/results/checkpoint-250/optimizer.pt (deflated 9%)\n","output_type":"stream"}]},{"cell_type":"code","source":"!zip -r new_model.zip /kaggle/working/mistral_7b_guanaco","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:29:59.084522Z","iopub.execute_input":"2023-12-12T00:29:59.084816Z","iopub.status.idle":"2023-12-12T00:30:19.811207Z","shell.execute_reply.started":"2023-12-12T00:29:59.084789Z","shell.execute_reply":"2023-12-12T00:30:19.810101Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"  adding: kaggle/working/mistral_7b_guanaco/ (stored 0%)\n  adding: kaggle/working/mistral_7b_guanaco/adapter_config.json (deflated 50%)\n  adding: kaggle/working/mistral_7b_guanaco/README.md (deflated 65%)\n  adding: kaggle/working/mistral_7b_guanaco/adapter_model.safetensors (deflated 8%)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Inference\n\nOnce the model is saved it can be use using pipeline functions. Also the input must be formatted to be given as an input to the model for inferencing.","metadata":{}},{"cell_type":"code","source":"logging.set_verbosity(logging.CRITICAL)\n\nprompt = \"How do I find true love?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:06:11.371768Z","iopub.execute_input":"2023-12-12T00:06:11.372745Z","iopub.status.idle":"2023-12-12T00:08:50.678676Z","shell.execute_reply.started":"2023-12-12T00:06:11.372705Z","shell.execute_reply":"2023-12-12T00:08:50.677616Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] How do I find true love? [/INST] Finding true love is a complex and personal journey, and there is no one-size-fits-all answer. However, here are some tips that may help you on your journey:\n\n1. Be yourself: The most important thing is to be true to yourself and your values. Don't try to be someone else or change who you are to please others.\n\n2. Be open-minded: Don't limit yourself to a certain type of person or relationship. Be open to new experiences and possibilities.\n\n3. Be patient: Finding true love takes time and effort. Don't rush the process or settle for less than what you want.\n\n4. Be active: Get out there and meet new people. Join clubs, go to events, and try new things.\n\n5. Be honest: Be honest with yourself and others about your feelings and intentions.\n","output_type":"stream"}]},{"cell_type":"code","source":"prompt = \"What is Datacamp Career track?\"\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"execution":{"iopub.status.busy":"2023-12-12T00:27:21.803818Z","iopub.execute_input":"2023-12-12T00:27:21.804713Z","iopub.status.idle":"2023-12-12T00:29:58.962054Z","shell.execute_reply.started":"2023-12-12T00:27:21.804670Z","shell.execute_reply":"2023-12-12T00:29:58.961102Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n","output_type":"stream"},{"name":"stdout","text":"<s>[INST] What is Datacamp Career track? [/INST] Datacamp Career Track is a program that provides a comprehensive learning path for individuals who want to become data scientists. It covers a wide range of topics, including data analysis, machine learning, and data visualization. The program is designed to help individuals develop the skills and knowledge needed to succeed in the field of data science. \n\nThe program consists of a series of courses, each of which is designed to build upon the knowledge and skills learned in the previous course. The courses are delivered through a combination of video lectures, interactive exercises, and quizzes. The program also includes a final project, where students apply the skills they have learned to a real-world problem. \n\nThe program is designed to be completed in approximately 12 weeks, but students can take longer if needed. Upon completion of the program, students receive a certificate of completion from Datacamp. \n\nOver\n","output_type":"stream"}]}]}